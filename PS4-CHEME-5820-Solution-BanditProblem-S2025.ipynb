{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0930b5-bb8a-4e6d-acf6-1c43765338af",
   "metadata": {},
   "source": [
    "# PS4: A Contextual Stochastic Bandit Personal Shopper\n",
    "Fill me in\n",
    "\n",
    "## Consumer choice problem\n",
    "Imagine that a consumer must choose $m$ possible goods (where each good is in a category with $k$ alternatives) $\\mathbf{n} = \\left\\{n_{1},n_{2},\\dots,n_{m}\\right\\}$, where $n_{j}\\in\\mathbb{R}_{\\geq\\epsilon}$ is the quantity of good $j$ chosen where $\\epsilon>0$, i.e., the consumer must choose at least $\\epsilon$ units of any good. Different combinations of goods are _scored_ using a utility function $U:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}$. In this case, let's assume our consumer uses [the Cobb-Douglas](https://en.wikipedia.org/wiki/Cobb%E2%80%93Douglas_production_function) utility:\n",
    "$$\n",
    "\\begin{align*}\n",
    "U(\\mathbf{n}) = \\prod_{i=1}^{m}n_{i}^{\\gamma_{i}}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\gamma_{i} = \\left\\{-1,1\\right\\}$ denote _user sentiment_ parameters: if $\\gamma_{j} = 1$, the good $j$ is preferred, otherwise is $\\gamma_{j} = -1$ good $j$ is _not_ preferred. Finally, the choice of goods $n_{1},\\dots,n_{m}$ is subject to a budget constraint:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{m}n_{i}C_{i} \\leq B\n",
    "\\end{align*}\n",
    "$$\n",
    "where $C_{i}$ is the unit cost of good $i$, and $B$ is the total budget the consumer can spend. The objective of a consumer is to maximize the utility of their choice (the combination of goods) subject to a budget constraint.\n",
    "\n",
    "## Stochastic Multi-Armed Bandits\n",
    "In the stochastic multi-armed bandit problem, an agent must choose an action $a$ from the set of all possible actions $\\mathcal{A}$, where $\\dim\\mathcal{A} = K$ during each round $t = 1,2,\\dots, T$ of a decision task. The agent chooses action $a\\in\\mathcal{A}$ and receives a reward $r_{a}$ from the environment, where $r_{a}$ is sampled from some (unknown) distribution $\\mathcal{D}_{a}$.\n",
    "\n",
    "For $t = 1,2,\\dots,T$:\n",
    "1. _Aggregator_: The agent picks an action $a_{t} \\in \\mathcal{A}$ at time time $t$. How the agent makes this choice is one of the main differences between the different algorithms for solving this problem. \n",
    "2. _Adversary_: The agent implements action $a_{t}$ and receives a (random) reward $r_{a}\\sim\\mathcal{D}_{a}$ where $r_{t}\\in\\left[0,1\\right]$. The distribution $\\mathcal{D}_{a}$ is only known to the adversary.\n",
    "3. The agent updates its _memory_ with the reward and continues to the next decision task. \n",
    "\n",
    "The agent is interested in learning the mean of the reward distribution of each arm, $\\mu(a) = \\mathbb{E}\\left[r_{t}\\sim\\mathcal{D}_{a}\\right]$, by experimenting against the world (adversary). \n",
    "* __Goal__: The goal of the agent is to maximize the total reward. However, the goal of the algorithm designer is to minimize the _regret_ of the algorithm that the agent uses to choose $a\\in\\mathcal{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb11a8d-98f9-44b7-b47f-10eedaeae119",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8833f121-3387-4477-9830-24115c8abbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03d3008-fda1-4172-8af4-fc177c949a15",
   "metadata": {},
   "source": [
    "First, let's build the `world(...)` function. \n",
    "* This function takes the action vector `a::Array{Int64,1}` (the indexes of the goods chosen from each of the $m$ categories), the amount of each good selected from each category from our agent, and returns the reward (utility) associated with selecting this action, i.e., $r\\sim\\mathcal{D}_{a}$. We'll use [a Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) and the Cobb-Douglas utility to model the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51eead03-b084-4d87-8560-b41fb212c6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "function world(a::Vector{Int64}, n::Dict{Int,Array{Float64,1}}, context::MyBanditConsumerContextModel)::Float64\n",
    "\n",
    "    # initialize -\n",
    "    γ = context.γ; # consumer preferences (unknown to bandits)\n",
    "    σ = context.σ; # noise in utility calculation (unknown to bandits)\n",
    "    B = context.B; # max budget (unknown to bandits)\n",
    "    C = context.C; # unit costs of goods (unknown to bandits)\n",
    "    λ = context.λ; # sensitivity to the budget\n",
    "    Z = context.Z; # noise model\n",
    "    ϵ = 0.001; # min unit required\n",
    "    number_of_categories = context.m; # number of categories\n",
    "\n",
    "    # compute the reward for this choice -\n",
    "    Ū = 1.0;\n",
    "    BC = 0.0;\n",
    "    for i ∈ 1:number_of_categories\n",
    "        \n",
    "        # what action in category i, did we just take?\n",
    "        aᵢ = a[i]; # this is which good to purchase in category i -\n",
    "        nᵢ = max(ϵ, n[i][aᵢ]); # this is how much of good i to purchase (must be geq ϵ)\n",
    "        Cᵢ = C[i][aᵢ]; # cost of chosen good in category i\n",
    "        γᵢ = γ[i][aᵢ]; # preference of good in category i\n",
    "        σᵢ = (σ[i][aᵢ]); # standard dev for good i\n",
    "        Zᵢ = Z[i]; # noise model\n",
    "   \n",
    "        # update the utility -\n",
    "        Ū *= nᵢ^(γᵢ)\n",
    "\n",
    "        # compute the budget constraint -\n",
    "        BC += nᵢ*Cᵢ;\n",
    "\n",
    "        # @show i, aᵢ, nᵢ, Cᵢ, BC, Ū;\n",
    "\n",
    "    end\n",
    "\n",
    "    # compute the budget constraint violation -\n",
    "    U = Ū - λ*max(0.0, (BC-B))^2; # use a penalty method to capture budget constraint\n",
    "\n",
    "    # return the reward -\n",
    "    return U;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e7284",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Finally, let's set some constants we'll use in the subsequent tasks. See the comment beside the value for a description of what it is, its permissible values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3943a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10000; # number of rounds for each decision task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b3cec-9420-499f-85be-d9f1bfa017bc",
   "metadata": {},
   "source": [
    "## Task 2: Something will go here.\n",
    "Fill me in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47292ca-6238-44aa-bc24-80bc82b5fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = let\n",
    "\n",
    "    # initialize -\n",
    "    γ = Dict{Int,Vector{Float64}}(); # consumer preferences (unknown to bandits)\n",
    "    σ = Dict{Int,Vector{Float64}}(); # noise in utility calculation (unknown to bandits)\n",
    "    B = 100.0; # max budget (unknown to bandits)\n",
    "    C = Dict{Int,Vector{Float64}}(); # unit costs of goods (unknown to bandits)\n",
    "    λ = 0.0; # sensitivity to the budget\n",
    "    Z = Dict{Int,Normal}(); # noise model\n",
    "    number_of_categories = 3; # number of categories\n",
    "\n",
    "    # set the parameters -\n",
    "    # preferences\n",
    "    γ[1] = [1.0, 1.0, 1.0]; # category 1\n",
    "    γ[2] = [1.0, -1.0, 1.0, 1.0, 1.0, 1.0]; # category 2\n",
    "    γ[3] = [1.0, 1.0, 1.0, 1.0]; # category 3\n",
    "\n",
    "    # uncertainty\n",
    "    σ[1] = [0.01, 0.01, 0.01]; # category 1\n",
    "    σ[2] = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]; # category 2\n",
    "    σ[3] = [0.01, 0.01, 0.01, 0.01]; # category 3\n",
    "\n",
    "    # costs\n",
    "    C[1] = [10.0, 20.0, 30.0]; # category 1\n",
    "    C[2] = [10.0, 20.0, 30.0, 40.0, 50.0, 60.0]; # category 2\n",
    "    C[3] = [10.0, 20.0, 30.0, 40.0]; # category 3\n",
    "\n",
    "    # noise model\n",
    "    Z[1] = Normal(0.0, 1.0); # category 1\n",
    "    Z[2] = Normal(0.0, 1.0); # category 2\n",
    "    Z[3] = Normal(0.0, 1.0); # category 3\n",
    "\n",
    "    # build a context model with the reqired parameters -\n",
    "    context = build(MyBanditConsumerContextModel, (\n",
    "        γ = γ, # consumer preferences (unknown to bandits)\n",
    "        σ = σ, # noise in utility calculation (unknown to bandits)\n",
    "        B = B, # max budget (unknown to bandits)\n",
    "        C = C, # unit costs of goods (unknown to bandits)\n",
    "        λ = λ, # sensitivity to the budget\n",
    "        Z = Z, # noise model\n",
    "        m = number_of_categories\n",
    "    )); # build the context\n",
    "\n",
    "    # return \n",
    "    context;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8680fd-798b-439d-ae82-e50dab5bd4b1",
   "metadata": {},
   "source": [
    "## Task 2: Evaluation of Algorithms\n",
    "In this task, we'll run the $\\epsilon$-greedy algorithm on our example `K`-arm bandit with categories problem. Let's start with a quick review of the $\\epsilon$-greedy algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004b72a-e76f-4caa-9956-39d430284308",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy Exploration\n",
    "One issue with the _uniform exploration_ algorithm is that it may not be the best choice for all problems. For example, the performance in the exploration phase may be _bad_ if many of the arms have a large gap $\\Delta({a})$:\n",
    "* _What is this gap_? Let the (true) mean reward for each arm be $\\mu(a) = \\mathbb{E}\\left[r_{t}\\sim\\mathcal{D}_{a}\\right]$, where $a\\in\\mathcal{A}$. The _best_ mean reward over the actions is $\\mu^{\\star} = \\max_{a\\in\\mathcal{A}}\\mu_{a}$. Then, the gap $\\Delta({a}) = \\mu^{\\star} - \\mu(a)$ is the difference between the mean reward of the best arm and the mean reward of arm $a$. If the gap is _large_, the agent may miss out on many rewards by exploring each arm equally.\n",
    "\n",
    "In a large gap, it may be better to spread out (and interleave) the exploration and exploitation phases of the arms. This is the idea behind the _epsilon-greedy_ algorithm. In this algorithm, the agent chooses the best arm with probability $1-\\epsilon$ and a random arm with probability $\\epsilon$. This allows the agent to explore the arms more evenly and may lead to better performance in cases where the gap is large.\n",
    "\n",
    "While [Slivkins](https://arxiv.org/abs/1904.07272) doesn't give a reference for the epsilon-greedy algorithm, other sources point to (at least in part) to [Thompson and Thompson sampling, proposed in 1933 in the context of drug trials](https://arxiv.org/abs/1707.02038).\n",
    "\n",
    "#### Epsilon-Greedy Algorithm\n",
    "The agent has $K$ arms (choices), $\\mathcal{A} = \\left\\{1,2,\\dots,K\\right\\}$, and the total number of rounds is $T$. The agent uses the following algorithm to choose which arm to pull (which action to take) during each round:\n",
    "\n",
    "For $t = 1,2,\\dots,T$:\n",
    "1. _Initialize_: Roll a random number $p\\in\\left[0,1\\right]$ and compute a threshold $\\epsilon_{t}\\sim{t}^{-1/3}$. Note, in other sources, $\\epsilon$ is a constant, not a function of $t$.\n",
    "2. _Exploration_: If $p\\leq\\epsilon_{t}$, choose a random (uniform) arm $a_{t}\\in\\mathcal{A}$. Execute the action $a_{t}$ and receive a reward $r_{t}$ from the _adversary_ (nature). \n",
    "3. _Exploitation_: Else if $p>\\epsilon_{t}$, choose action $a^{\\star}$ (action with the highest average reward so far, the greedy choice). Execute the action $a^{\\star}_{t}$ and recieve a reward $r_{t}$ from the _adversary_ (nature).\n",
    "4. Update list of rewards for $a_{t}\\in\\mathcal{A}$\n",
    "\n",
    "__Theorem__: The epsilon-greedy algowithm with exploration probability $\\epsilon_{t}={t^{-1/3}}\\cdot\\left(K\\cdot\\log(t)\\right)^{1/3}$ achives a regret bound of $\\mathbb{E}\\left[R(t)\\right]\\leq{t}^{2/3}\\cdot\\left(K\\cdot\\log(t)\\right)^{1/3}$ for each round $t$.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2242e",
   "metadata": {},
   "source": [
    "We've modified the base algorithm to work category-wise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2aab1ab-1813-4d78-a9f5-2abb60e2c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, model = let\n",
    "\n",
    "    # initialize -\n",
    "    K = Dict{Int64,Int64}(); # arms dictionary\n",
    "    n = Dict{Int64, Array{Float64,1}}() # items dictionary\n",
    "\n",
    "    # How many alternatives (arms) do we have in category?\n",
    "    K[1] = 3; # category 1 has three possible choices\n",
    "    K[2] = 6; # categorty 2 has six possible choices\n",
    "    K[3] = 4; # category 4 has four possible choices\n",
    "\n",
    "    # how many items would we purchase *if* we choose alternative i in category j?  \n",
    "    n[1] = [1.0, 1.0, 1.0]; # category 1\n",
    "    n[2] = [2.0, 2.0, 2.0, 2.0, 2.0, 2.0]; # category 2\n",
    "    n[3] = [3.0, 3.0, 3.0, 3.0]; # category 3\n",
    "    \n",
    "    # build model -\n",
    "    m = build(MyEpsilonGreedyAlgorithmModel, (\n",
    "        K = K, # arms dictionary\n",
    "        n = n, # items dictionary\n",
    "    ));\n",
    "    \n",
    "    # run the scenario, let's see what happens\n",
    "    results = solve(m, T = T, world = world, context=context);\n",
    "    \n",
    "    # return -\n",
    "    results, m;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a98322",
   "metadata": {},
   "source": [
    "__What does the modified algorithm produce__? Let's build a table to see that choices that the bandits in each category are making. `Unhide` the code-block below to see how we construct the simulation results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c47adb9-87fa-47a2-a82e-9b12e67d401b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== ======== ========= ========== ========= =========== ==========\n",
      " \u001b[1m category \u001b[0m \u001b[1m action \u001b[0m \u001b[1m       n \u001b[0m \u001b[1m unitcost \u001b[0m \u001b[1m   spend \u001b[0m \u001b[1m remaining \u001b[0m \u001b[1m       Ū \u001b[0m\n",
      " \u001b[90m    Int64 \u001b[0m \u001b[90m  Int64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m   Float64 \u001b[0m \u001b[90m Float64 \u001b[0m\n",
      "=========== ======== ========= ========== ========= =========== ==========\n",
      "         1        2       1.0       20.0      20.0        80.0       1.0\n",
      "         2        1       2.0       10.0      40.0        60.0       2.0\n",
      "         3        4       3.0       40.0     160.0       -60.0       6.0\n",
      "=========== ======== ========= ========== ========= =========== ==========\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    \n",
    "    # initialize -\n",
    "    df = DataFrame();\n",
    "    number_of_categories = context.m; # number of categories\n",
    "    category_action_map = model.K # get the number of arms\n",
    "    B = context.B; # max budget (unknown to bandits)\n",
    "    \n",
    "\n",
    "    # loop over the categories -\n",
    "    BC = 0.0;\n",
    "    U = 1.0;\n",
    "    for i ∈ 1:number_of_categories\n",
    "    \n",
    "        # Data for this categorty\n",
    "        K = category_action_map[i]; # get the number of arms for this category\n",
    "        data = results[i]; # get the data for this category\n",
    "\n",
    "        μ = Array{Float64,1}(undef, K); # mean of the data\n",
    "        for j ∈ 1:K\n",
    "            μ[j] = filter(x -> x != 0.0, data[:,j]) |> x-> mean(x)\n",
    "        end\n",
    "       \n",
    "        # which action should we take?\n",
    "        aᵢ = argmax(μ); # this is which good to purchase in category i -\n",
    "        BC += model.n[i][aᵢ]*context.C[i][aᵢ]; # budget constraint\n",
    "        U *= model.n[i][aᵢ]^(context.γ[i][aᵢ]); # update the utility\n",
    "\n",
    "        row_df = (\n",
    "            category = i,\n",
    "            action = aᵢ,\n",
    "            n = model.n[i][aᵢ], # this is how much of good i to purchase (must be geq ϵ)\n",
    "            unitcost = context.C[i][aᵢ], # cost of chosen good in category i\n",
    "            spend = BC, # this is how much we spent\n",
    "            remaining = B - BC, # budget constraint\n",
    "            Ū = U, # utility\n",
    "        );\n",
    "        \n",
    "        # add the row to the dataframe -\n",
    "        push!(df, row_df);\n",
    "    end\n",
    "\n",
    "    pretty_table(df, tf = tf_simple);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a63efe67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
